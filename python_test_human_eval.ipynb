{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ffd3a00",
   "metadata": {},
   "source": [
    "# initial tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bce0da9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /Users/watsonjo/.pyenv/versions/3.7.16/lib/python3.7/site-packages (0.21.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2726ad21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import requests\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "API_KEY=os.getenv(\"API_KEY\")\n",
    "# MODEL=\"gpt-3.5-turbo-0125\"\n",
    "MODEL=\"gpt-4-0125-preview\"\n",
    "additional_context = True\n",
    "previous_attempts: True\n",
    "\n",
    "def listModels():\n",
    "    url = \"https://api.openai.com/v1/models\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": \"Bearer \" + API_KEY,\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    print(response.json())\n",
    "def gpt(code, additional_context = '', error = '', ae = '', previous_code = ''):\n",
    "    url = \"https://api.openai.com/v1/chat/completions\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": \"Bearer \" + API_KEY,\n",
    "    }\n",
    "    context = \"You are a tool used to automatically generate source code for unit tests. Source code for a function that is to be tested will be provided and you should generate an appropriate unit test that can be ran to ensure the code is correct. Code input will be in Python and you should respond with a Python code block containing the unit test as output. Do not use any testing frameworks. Do not start output with \\\"```python\\\". Don't use Parametrize. For numeric equality assertions, allow a tolerance of 1e^-5. Ensure the file can be run directly using a check for __name__==\\\"main\\\". Add assertion messages for failures. Do not include the original function in the response.\"\n",
    "    if additional_context and additional_context:\n",
    "        context += \" Some additional context about the method under test is \\\"\"+additional_context+\"\\\".\"\n",
    "    if previous_code and previous_attempts:\n",
    "        context += \" Previously, you generated \\\"\"+previous_code+\"\\\".\"\n",
    "    if error and previous_attempts:\n",
    "        context += \" This was executed and the following was the result \\\"\"+error+\"\\\". Provide a new solution in the same format, which fixes the problems.\"\n",
    "    if ae and previous_attempts:\n",
    "        context += \" This was executed and there was a failed assertion \\\"\"+ae+\"\\\". Try fix the test for the failed assertion, leaving all the passing assertions unchanged and return the updated result.\"\n",
    "    data = {\n",
    "        \"model\": MODEL,\n",
    "        \"messages\": [\n",
    "          {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": context\n",
    "          },\n",
    "          {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"\"\"\n",
    "              def add(a, b):\n",
    "                return a + b\"\n",
    "              \"\"\"\n",
    "          },\n",
    "          {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"def test_add():\\n    assert add(3, 5) == 8, \\\"Test case 1 failed\\\"\\n    assert add(-3, -5) == -8, \\\"Test case 2 failed\\\"\\n    assert add(3, -5) == -2, \\\"Test case 3 failed\\\"\\nif __name__ == \\\"__main__\\\":\\n    test_add()\"\n",
    "          },\n",
    "          {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": code\n",
    "          },\n",
    "        ],\n",
    "        \"max_tokens\": 1000,\n",
    "        \"temperature\": 1.0,\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    output = response.json()['choices'][0]['message']['content']\n",
    "\n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c7dcf961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['HumanEval/0, passes(attempt 1)', 'HumanEval/1, passes(attempt 1)', 'HumanEval/2, passes(attempt 1)'], []]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import traceback\n",
    "import re\n",
    "SAMPLES_TO_RUN = 10\n",
    "ADDITIONAL_ATTEMPTS = 2\n",
    "out = [[],[]]\n",
    "count = 0\n",
    "def exec_code(eval_program, prompt, check_program, out, attempts=0):\n",
    "    attempts += 1\n",
    "    error = ''\n",
    "    ae = ''\n",
    "    try:\n",
    "        exec(check_program)\n",
    "        out[0].append(f\"{d['task_id']}, passes(attempt {attempts})\")\n",
    "        return\n",
    "    except AssertionError as err:\n",
    "        tb = traceback.format_exc()\n",
    "\n",
    "        print(\"Failing Test\\n\\n\")\n",
    "        print(\"\\'''\\n\" + check_program + \"\\n'''\")\n",
    "        out[1].append(f\"{d['task_id']}, failed assertion\")\n",
    "        try:\n",
    "            ae = re.match(r'.*(AssertionError: .*)', tb.replace(\"\\n\",\"\")).group(1)\n",
    "        except Exception as f:\n",
    "            ae = err\n",
    "        print(ae)\n",
    "    except Exception as e:\n",
    "        out[1].append(f\"{d['task_id']} - attempt {attempts}, failed to run\")\n",
    "        print(\"\\'''\\n\" + check_program + \"\\n'''\")\n",
    "        print(str(e))\n",
    "        error = str(e)\n",
    "    if attempts > ADDITIONAL_ATTEMPTS:\n",
    "        return\n",
    "    next_attempt = gpt(eval_program, prompt, error, ae, check_program)\n",
    "    check_program = (\n",
    "                next_attempt + \"\\n\"\n",
    "            )\n",
    "    return exec_code(eval_program, prompt, check_program, out, attempts)\n",
    "\n",
    "with open('HumanEval.jsonl') as file:\n",
    "    for line in file:\n",
    "        count += 1\n",
    "        d = json.loads(line)\n",
    "        d['soln'] = gpt(d['canonical_solution'], d[\"prompt\"])\n",
    "#         d['soln'] = \"def has_close_elements(numbers, threshold):\\n    for idx, elem in enumerate(numbers):\\n        for idx2, elem2 in enumerate(numbers):\\n            if idx != idx2:\\n                distance = abs(elem - elem2)\\n                if distance < threshold:\\n                    return True\\n    return False\\ndef test_has_close_elements():\\n    assert has_close_elements([1.0, 2.0, 3.0], 0.5) == False, \\\"Test case 1 failed\\\"\\n    assert has_close_elements([1.0, 2.8, 3.0], 0.21) == True, \\\"Test case 2 failed\\\"\\n    assert has_close_elements([0.1, 0.2, 0.3], 0.11) == True, \\\"Test case 3 failed\\\"\\n    assert has_close_elements([10, 20, 30], 10.1) == True, \\\"Test case 4 failed\\\"\\n    assert has_close_elements([100, 200, 300], 100.1) == True, \\\"Test case 5 failed\\\"\\n    assert has_close_elements([], 0.5) == False, \\\"Test case 6 failed\\\"\\n    assert has_close_elements([1.000001, 1.000002], 0.0000001) == False, \\\"Test case 7 failed\\\"\\n    assert has_close_elements([1.000001, 1.000002], 0.000001) == True, \\\"Test case 8 failed\\\"\\nif __name__ == \\\"__main__\\\":\\n    test_has_close_elements()\"\n",
    "        check_program = (\n",
    "                d[\"soln\"] + \"\\n\"\n",
    "            )\n",
    "        exec_code(d['canonical_solution'], d[\"prompt\"], check_program, out)\n",
    "        if count > SAMPLES_TO_RUN-1:\n",
    "            break\n",
    "        \n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "883fb710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HumanEval/0, passes(attempt 1)\n",
      "HumanEval/1, passes(attempt 1)\n",
      "HumanEval/2, passes(attempt 1)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import traceback\n",
    "import re\n",
    "import time    \n",
    "SAMPLES_TO_RUN = 3\n",
    "ADDITIONAL_ATTEMPTS = 2\n",
    "TEST_NAME = \"testing\"\n",
    "count = 0\n",
    "def exec_code(instance):\n",
    "#     eval_program, prompt, check_program, out, attempts=0\n",
    "    error = ''\n",
    "    ae = ''\n",
    "    check_program = (\n",
    "                instance['prompt'] + \n",
    "                instance['canonical_solution'] + \"\\n\" + \n",
    "                instance['test'] + \"\\n\"\n",
    "            )\n",
    "    try:\n",
    "        exec(check_program)\n",
    "        instance[\"runs\"] = True\n",
    "        instance[\"passes\"] = True\n",
    "        print(f\"{instance['task_id']}, passes(attempt {instance['attempts_made']})\")\n",
    "        return instance\n",
    "    except AssertionError as err:\n",
    "        tb = traceback.format_exc()\n",
    "        instance['runs'] = True\n",
    "        instance['passes'] = False\n",
    "        print(f\"{instance['task_id']} - attempt {instance['attempts_made']}, failed assertion\")\n",
    "        try:\n",
    "            ae = re.match(r'.*(AssertionError: .*)', tb.replace(\"\\n\",\"\")).group(1)\n",
    "        except Exception as f:\n",
    "            ae = err\n",
    "        instance['error'] = ae\n",
    "    except Exception as e:\n",
    "        instance['runs'] = False\n",
    "        instance['passes'] = False\n",
    "        print(f\"{instance['task_id']} - attempt {instance['attempts_made']}, failed to run\")\n",
    "#         print(\"\\'''\\n\" + check_program + \"\\n'''\")\n",
    "#         print(str(e))\n",
    "        error = str(e)\n",
    "        instance['error'] = error\n",
    "    if instance['attempts_made'] > ADDITIONAL_ATTEMPTS:\n",
    "        return instance\n",
    "    next_instance = {\n",
    "        'task_id': instance['task_id'],\n",
    "        'prompt': instance['prompt'],\n",
    "        'canonical_solution': instance['canonical_solution'],\n",
    "        'attempts_made': instance['attempts_made']+1,\n",
    "        'previous_attempt': instance\n",
    "    }\n",
    "#     next_attempt = \"      def has_close_elemnts(numbers, threshold):\\n    for idx, elem in enumerate(numbers):\\n        for idx2, elem2 in enumerate(numbers):\\n            if idx != idx2:\\n                distance = abs(elem - elem2)\\n                if distance < threshold:\\n                    return True\\n    return False\\ndef test_has_close_elements():\\n    assert has_close_elements([1.0, 2.0, 3.0], 0.5) == False, \\\"Test case 1 failed\\\"\\n    assert has_close_elements([1.0, 2.8, 3.0], 0.21) == True, \\\"Test case 2 failed\\\"\\n    assert has_close_elements([0.1, 0.2, 0.3], 0.11) == True, \\\"Test case 3 failed\\\"\\n    assert has_close_elements([10, 20, 30], 10.1) == True, \\\"Test case 4 failed\\\"\\n    assert has_close_elements([100, 200, 300], 100.1) == True, \\\"Test case 5 failed\\\"\\n    assert has_close_elements([], 0.5) == False, \\\"Test case 6 failed\\\"\\n    assert has_close_elements([1.000001, 1.000002], 0.0000001) == False, \\\"Test case 7 failed\\\"\\n    assert has_close_elements([1.000001, 1.000002], 0.000001) == True, \\\"Test case 8 failed\\\"\\nif __name__ == \\\"__main__\\\":\\n    test_has_close_elements()\"\n",
    "#     if instance['attempts_made'] > 1:\n",
    "#         next_attempt = \"def has_close_elements(numbers, threshold):\\n    for idx, elem in enumerate(numbers):\\n        for idx2, elem2 in enumerate(numbers):\\n            if idx != idx2:\\n                distance = abs(elem - elem2)\\n                if distance < threshold:\\n                    return True\\n    return False\\ndef test_has_close_elements():\\n    assert has_close_elements([1.0, 2.0, 3.0], 0.5) == False, \\\"Test case 1 failed\\\"\\n    assert has_close_elements([1.0, 2.8, 3.0], 0.21) == True, \\\"Test case 2 failed\\\"\\n    assert has_close_elements([0.1, 0.2, 0.3], 0.11) == True, \\\"Test case 3 failed\\\"\\n    assert has_close_elements([10, 20, 30], 10.1) == True, \\\"Test case 4 failed\\\"\\n    assert has_close_elements([100, 200, 300], 100.1) == True, \\\"Test case 5 failed\\\"\\n    assert has_close_elements([], 0.5) == False, \\\"Test case 6 failed\\\"\\n    assert has_close_elements([1.000001, 1.000002], 0.0000001) == False, \\\"Test case 7 failed\\\"\\nif __name__ == \\\"__main__\\\":\\n    test_has_close_elements()\"\n",
    "    next_attempt = gpt(instance['canonical_solution'], instance[\"prompt\"], error, ae, instance[\"test\"])\n",
    "    next_instance['test'] = next_attempt\n",
    "    return exec_code(next_instance)\n",
    "\n",
    "with open('HumanEval.jsonl') as file:\n",
    "    timestamp = time.time()\n",
    "    filename = f\"results/{TEST_NAME}_{timestamp}.jsonl\"\n",
    "    open(filename, \"x\")\n",
    "    for line in file:\n",
    "        count += 1\n",
    "        instance = json.loads(line)\n",
    "        instance['attempts_made'] = 1\n",
    "        instance['test'] = gpt(instance['canonical_solution'], instance[\"prompt\"])\n",
    "#         instance['test'] = \"def has_close_elements(numbers, threshold):\\n    for idx, elem in enumerate(numbers):\\n        for idx2, elem2 in enumerate(numbers):\\n            if idx != idx2:\\n                distance = abs(elem - elem2)\\n                if distance < threshold:\\n                    return True\\n    return False\\ndef test_has_close_elements():\\n    assert has_close_elements([1.0, 2.0, 3.0], 0.5) == False, \\\"Test case 1 failed\\\"\\n    assert has_close_elements([1.0, 2.8, 3.0], 0.21) == True, \\\"Test case 2 failed\\\"\\n    assert has_close_elements([0.1, 0.2, 0.3], 0.11) == True, \\\"Test case 3 failed\\\"\\n    assert has_close_elements([10, 20, 30], 10.1) == True, \\\"Test case 4 failed\\\"\\n    assert has_close_elements([100, 200, 300], 100.1) == True, \\\"Test case 5 failed\\\"\\n    assert has_close_elements([], 0.5) == False, \\\"Test case 6 failed\\\"\\n    assert has_close_elements([1.000001, 1.000002], 0.0000001) == False, \\\"Test case 7 failed\\\"\\n    assert has_close_elements([1.000001, 1.000002], 0.000001) == True, \\\"Test case 8 failed\\\"\\nif __name__ == \\\"__main__\\\":\\n    test_has_close_elements()\"\n",
    "        out = exec_code(instance)\n",
    "        with open(filename, 'a') as file:\n",
    "            json.dump(out, file)\n",
    "            file.write('\\n')\n",
    "        if count > SAMPLES_TO_RUN-1:\n",
    "            break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220d563a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
